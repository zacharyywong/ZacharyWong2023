{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75f97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script extracts the title, link, and short description of search results on Google \n",
    "\n",
    "#import all libraries\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time, sys, requests, random\n",
    "\n",
    "\n",
    "# import libs, authorize gspread  \n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "google_key_file = '/Users/zacharywong/Documents/ServiceAccountKey-Secret/pelagic-tracker-338302-eaf0e0e671cb.json'\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(google_key_file, scope)\n",
    "gc = gspread.authorize(credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d63fc812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# paths/baseurls\n",
    "spreadsheet_id = '1vFXonFCyUlEKa1f0s5tvHCKeTek_sAv7rUPYfYss0Qo'\n",
    "companyFilePath = '/Users/zacharywong/github/zacharywong2023/DigitalHealthWebscrape/Companies/DigitalHealthWebscrape - Companies.csv'\n",
    "driver_path = '/Users/zacharywong/Documents/Work/Portfolio/DigitalHealthWebscrape/chromedriver'\n",
    "pathtoFile = '/Users/zacharywong/github/zacharywong2023/DigitalHealthWebscrape/Deliverables/'\n",
    "googleurl = 'https://www.google.com/'\n",
    "\n",
    "columnName = 'Company Names (Leave Blank if Using LinkedIn Query - See Previous Tab)'\n",
    "#ints \n",
    "adjustDenominator = 3\n",
    "maxbackOff = 120\n",
    "maxResult = 1\n",
    "waitTime = 5\n",
    "waitRun = 0.3\n",
    "\n",
    "#bools\n",
    "useURL = False\n",
    "useName = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a761739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait():\n",
    "    pass\n",
    "    global waitRun\n",
    "    sleepTime = waitRun + random.uniform(0, 1)\n",
    "    #print('sleepTime: ', sleepTime)\n",
    "    time.sleep(sleepTime)\n",
    "    waitRun = waitRun*2\n",
    "    #print('waitRun: ', waitRun)\n",
    "    if (waitRun >= maxbackOff):\n",
    "        sys.exit(\"error: read from sheets quota exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2780835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: reads in values from DigitalHealthWebscrape google sheet\n",
    "# Need spreadsheet ID and the cell address where the value should be read in \n",
    "# returns the value \n",
    "\n",
    "def readinValue(cellLocation, sheetIndex):\n",
    "    sh = gc.open_by_key(spreadsheet_id)\n",
    "    worksheet = sh.get_worksheet(sheetIndex)\n",
    "    try:\n",
    "        value = worksheet.acell(cellLocation).value\n",
    "    except:\n",
    "        wait()\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84ee9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCompanies():\n",
    "    #links = []\n",
    "    names = []\n",
    "    df = pd.read_csv(companyFilePath, usecols = [columnName])\n",
    "    df[columnName]=df[columnName].fillna(' ')\n",
    "    #df['Link']=df['Link'].fillna(' ')\n",
    "    names = df[columnName].tolist()\n",
    "    #links = df['Link'].tolist()\n",
    "    print(df)\n",
    "    return names \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6bf564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readKeyWords():\n",
    "    keywords = []\n",
    "    row = 2;\n",
    "    sheetIndex = 2\n",
    "    isDone = False\n",
    "    cellLocationColumn = 'B'\n",
    "    while (isDone == False):\n",
    "        cellLocationKeyWords = cellLocationColumn + str(row)\n",
    "        try:\n",
    "            keyword = readinValue(cellLocationKeyWords, sheetIndex)\n",
    "            keywords.append(keyword)\n",
    "            if(keyword == None):\n",
    "                isDone = True\n",
    "                break\n",
    "            else:\n",
    "                row += 1 \n",
    "        except:\n",
    "            wait()\n",
    "    keywords = keywords[0:len(keywords)-1]\n",
    "    print(keywords)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19843095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCalculateLiklihood():\n",
    "    #print(links, names)\n",
    "    sheetIndex = 2\n",
    "    cellLocationLiklihood = 'A2'\n",
    "    calculateLiklihood = False\n",
    "    try:\n",
    "        calculateLiklihoodInput = readinValue(cellLocationLiklihood, sheetIndex)\n",
    "        if (calculateLiklihoodInput == 'Yes'):\n",
    "            calculateLiklihood = True\n",
    "        else:\n",
    "            calculateLiklihood = False\n",
    "    except:\n",
    "        wait()\n",
    "    return calculateLiklihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34a706f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInput():\n",
    "    names = readCompanies()\n",
    "    calculateLiklihood = readCalculateLiklihood()\n",
    "    keywords = readKeyWords()\n",
    "    return names, calculateLiklihood, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5294e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to extract links from 1 page of results\n",
    "def extractLinks(soup):\n",
    "    links = []\n",
    "    #Tags and classes\n",
    "    linksTag = 'div'\n",
    "    linksClass = 'yuRUbf'\n",
    "    linksAttr = 'href'\n",
    "    searchLinks = soup.find_all(linksTag, class_ = linksClass)\n",
    "    for h in searchLinks:\n",
    "        link = h.a.get(linksAttr)\n",
    "        links.append(link)\n",
    "    return links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6d4dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTitles(soup):\n",
    "    titles = [] \n",
    "    \n",
    "    titlesClass = 'LC20lb MBeuO DKV0Md'\n",
    "    titlesTag = 'h3'\n",
    "    searchTitles = soup.find_all(titlesTag, class_= titlesClass)\n",
    "    for h in searchTitles:\n",
    "        titles.append(h.text)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aa3778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to extract texts from 1 page of results\n",
    "def extractTexts(soup):\n",
    "    texts = []\n",
    "    textsClass = 'VwiC3b yXK7lf MUxGbd yDYNvb lyLwlc lEBKkf'\n",
    "    textsTag = 'div'\n",
    "    searchText = soup.find_all(textsTag, class_= textsClass)\n",
    "    for h in searchText:\n",
    "        fullText = h.text\n",
    "        try:\n",
    "            splitText = fullText.split('â€” ', 1)\n",
    "            text = splitText[1]\n",
    "            texts.append(text);\n",
    "        except:\n",
    "            texts.append(fullText)\n",
    "    return texts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "670c7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateLiklihoods(links, keywords):\n",
    "    #keywords = ['personalized', 'personalization', 'machine-learning', 'AI', 'Artificial Intelligence', '24/7', 'democratizing']\n",
    "    liklihoods = []\n",
    "    detectedWordsAll = []\n",
    "    ignore = ['[document]', 'a', 'article', 'label', 'script', 'style']\n",
    "    liklihoodDenom = len(keywords) - adjustDenominator\n",
    "    for url in links: \n",
    "        detectedWords = []\n",
    "        output = ''\n",
    "        count = 0\n",
    "        liklihood = 0\n",
    "\n",
    "        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        html_page = res.content\n",
    "        soup = bs(html_page, 'html.parser')\n",
    "        #print(\"Encoded method :\" + url + \": \", soup.original_encoding)\n",
    "        text = soup.find_all(text=True)\n",
    "       \n",
    "        for t in text:\n",
    "            if t.parent.name not in ignore:\n",
    "                output += '{} '.format(t)\n",
    "\n",
    "        # analyze the webpage to detect keywords \n",
    "        outputSub = output.split(' ')\n",
    "        #print(outputSub)\n",
    "        for word in keywords: \n",
    "            if (word in outputSub or word.capitalize() in outputSub or word + '\\n' in outputSub):\n",
    "                #print('\"' + word +'\"' + ' detected')\n",
    "                detectedWords.append(word)\n",
    "                count +=1 \n",
    "            #else:\n",
    "                #print(word + ' is not there')\n",
    "        liklihood = round((count / liklihoodDenom), 2)\n",
    "        print(liklihood)\n",
    "        liklihoods.append(liklihood)\n",
    "        detectedWordsAll.append(detectedWords)\n",
    "        #detectedWordsAll = ', '.join(str(keyword) for keyword in detectedWordsAll)\n",
    "    #print(\"detectedWordsAll\", detectedWordsAll)\n",
    "    return liklihoods, detectedWordsAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7add298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the keyword you want to search for depending on whether link or name is given\n",
    "# we find the search bar using its name attribute value (q)\n",
    "\n",
    "def searchGoogle(index, namesInput, googleurl, useURL, waitTime, driver):\n",
    "    #print(\"index: \", index)\n",
    "    #print(\"Number of Companies: \" + str((len(linksInput))))\n",
    "    #print(\"links: \", linksInput)\n",
    "    #print(\"names: \", namesInput)\n",
    "    #siteURL = linksInput[index]\n",
    "    #print(\"siteURL: \", siteURL)\n",
    "    websiteName = namesInput[index]\n",
    "    #print(\"websiteName: \", websiteName)\n",
    "    driver.get(googleurl)\n",
    "    searchBar = driver.find_element(By.NAME, 'q')\n",
    "    \n",
    "    # Booleans\n",
    "    #if (siteURL == ' '):\n",
    "    useURL = False; \n",
    "    useName = True; \n",
    "    #else:\n",
    "    #    useURL = True; \n",
    "     #   useName = False; \n",
    "\n",
    "    # first we send our keyword to the search bar followed by the enter # key depending on using URL or website name \n",
    "\n",
    "    if (useURL):\n",
    "        query = \"site: \" + siteURL\n",
    "        print('query: ' + query)\n",
    "        try:\n",
    "            searchBar.send_keys(query)\n",
    "            searchBar.send_keys('\\n')\n",
    "        except Exception as e : \n",
    "            WebDriverWait(driver, waitTime).until(EC.presence_of_element_located((By.NAME, 'q')))\n",
    "            searchBar.send_keys(query)\n",
    "            searchBar.send_keys('\\n')\n",
    "    else:\n",
    "        query = websiteName\n",
    "        print('query: ' + query)\n",
    "        searchBar.send_keys(query)\n",
    "        searchBar.send_keys('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f55f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture links, header, and text\n",
    "#pageInfo is a list of dictionaries for each page with keys/value pairs: header, link, text\n",
    "# extract and load each page of results to pageInfo \n",
    "def parseHTML(driver, calculateLiklihood, keywords, maxResult, pageInfo):\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    links = extractLinks(soup);\n",
    "    texts = extractTexts(soup);\n",
    "    titles = extractTitles(soup);\n",
    "    if (calculateLiklihood):\n",
    "        liklihoods, detectedWordsAll = calculateLiklihoods(links[0:1], keywords)\n",
    "        #print('detectedWordsAllStringForm: ' + str(detectedWordsAll))\n",
    "        #detectedWordsAll = ', '.join([str(keyword) for keyword in detectedWordsAll])\n",
    "        #print(\"detectedWordsAll String Form: \" + detectedWordsAll)\n",
    "        pageInfo = addToPageInfo(titles, links, texts, maxResult, pageInfo, liklihoods, detectedWordsAll)\n",
    "    else:\n",
    "        pageInfo = addToPageInfo(titles, links, texts, maxResult, pageInfo, liklihoods = None, detectedWordsAll = None)\n",
    "\n",
    "    return pageInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768eb19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to add 1 page of results to pageInfo list\n",
    "def addToPageInfo(titles, links, texts, maxResult, pageInfo, liklihoods, detectedWordsAll):\n",
    "    index = 0;\n",
    "    while (index < maxResult):\n",
    "        #print(\"texts: \", texts)\n",
    "        #print(\"liklihoods: \", liklihoods)\n",
    "        #print(\"length of links list: \" + str((len(linksInput)-1)))\n",
    "        #print(\"links: \", links)\n",
    "        #print(\"titles: \", titles)\n",
    "        #print('detectedWordsAll', detectedWordsAll)\n",
    "        #create new dictionary of each search results' title, link, and text\n",
    "        if (liklihoods == None):\n",
    "            pageInfo.append({\"Title\": titles[index], \"Link\": links[index], \"About\": texts[index]})\n",
    "        else:\n",
    "            #print(\"detectedWordsString: \", detectedWordsAll)\n",
    "            detectedWords = str(detectedWordsAll[index])\n",
    "            pageInfo.append({\"Percentage of Keywords Detected\": liklihoods[index], \"Title\": titles[index], \"Link\": links[index], \"About\": texts[index],  \"Detected KeyWords\": detectedWords})\n",
    "        index += 1\n",
    "    return pageInfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aded4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportCSV(df, pathtoFile, fileName):\n",
    "    # convert pageInfo to pandas dataframe and export as csv \n",
    "    df.to_csv(pathtoFile + fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d240d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSpreadSheet(df, sheetIndex):\n",
    "    sh = gc.open_by_key(spreadsheet_id)\n",
    "    worksheet = sh.get_worksheet(sheetIndex)\n",
    "    worksheet.clear()\n",
    "    worksheet.update([df.columns.values.tolist()] + df.values.tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28853f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportDeliverables(pageInfo, pathtoFile, fileName, calculateLiklihood):\n",
    "    df = pd.DataFrame(pageInfo)\n",
    "    if (calculateLiklihood):\n",
    "        df = df.sort_values(by = ['Percentage of Keywords Detected'], ascending=False)\n",
    "        df['Percentage of Keywords Detected'] = df['Percentage of Keywords Detected'] * 100\n",
    "    sheetIndex = 3\n",
    "    #print(df.columns.values.tolist())\n",
    "    #print(df.values.tolist())\n",
    "    #print(pageInfo)\n",
    "    exportCSV(df, pathtoFile, fileName)\n",
    "    updateSpreadSheet(df, sheetIndex)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40c073c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runExtraction():\n",
    "    index = 0\n",
    "    namesInput, calculateLiklihood, keywords = readInput()\n",
    "    print(\"Number of Companies: \" + str((len(namesInput))))\n",
    "    fileName = 'InputCompanies.csv'\n",
    "    # Access chromedriver and determine path \n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service = service)\n",
    "    \n",
    "    # list of dictionaries with key/value pairs: title, link, text\n",
    "    # Contains all information for all search results \n",
    "    pageInfo = []\n",
    "\n",
    "    while (index < len(namesInput)):\n",
    "        searchGoogle(index, namesInput, googleurl, useURL, waitTime, driver)\n",
    "        for page in range(0, 1):\n",
    "            pageInfo = parseHTML(driver, calculateLiklihood, keywords, maxResult, pageInfo)\n",
    "    #print(pageInfo)\n",
    "            index += 1\n",
    "        time.sleep(waitRun)\n",
    "    df = exportDeliverables(pageInfo, pathtoFile, fileName, calculateLiklihood)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f29f5bf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['Company Names (Leave Blank if Using LinkedIn Query - See Previous Tab)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gl/jgk7f2w528qgjggck6glqp200000gn/T/ipykernel_16337/3810104477.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# implement exponential backoff algorithm to prevent exceeding read quota from sheets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunExtraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mendTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtimeElapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendTime\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/gl/jgk7f2w528qgjggck6glqp200000gn/T/ipykernel_16337/3621112515.py\u001b[0m in \u001b[0;36mrunExtraction\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrunExtraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnamesInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculateLiklihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Companies: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamesInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'InputCompanies.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/gl/jgk7f2w528qgjggck6glqp200000gn/T/ipykernel_16337/625513735.py\u001b[0m in \u001b[0;36mreadInput\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadCompanies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcalculateLiklihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadCalculateLiklihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadKeyWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculateLiklihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/gl/jgk7f2w528qgjggck6glqp200000gn/T/ipykernel_16337/53359043.py\u001b[0m in \u001b[0;36mreadCompanies\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#links = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompanyFilePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#df['Link']=df['Link'].fillna(' ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             ):\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_usecols_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_validate_usecols_names\u001b[0;34m(self, usecols, names)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;34mf\"Usecols do not match columns, columns expected but not found: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m                 \u001b[0;34mf\"{missing}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['Company Names (Leave Blank if Using LinkedIn Query - See Previous Tab)']"
     ]
    }
   ],
   "source": [
    "# Run the script \n",
    "# implement exponential backoff algorithm to prevent exceeding read quota from sheets\n",
    "startTime = time.time()\n",
    "df = runExtraction()\n",
    "endTime = time.time()\n",
    "timeElapsed = endTime - startTime\n",
    "print('Time Elapsed: ', timeElapsed)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4553b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65104f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
