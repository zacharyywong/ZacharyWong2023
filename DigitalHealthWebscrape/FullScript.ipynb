{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a52af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script extracts the title, link, and short description of search results on Google \n",
    "\n",
    "#import all libraries\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "# import libs, authorize gspread  \n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "scope = [\n",
    "   'https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "google_key_file = '/Users/zacharywong/Documents/ServiceAccountKey-Secret/pelagic-tracker-338302-eaf0e0e671cb.json'\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(google_key_file, scope)\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "spreadsheet_id = '1vFXonFCyUlEKa1f0s5tvHCKeTek_sAv7rUPYfYss0Qo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7d6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "liklihoodDenom = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5900ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to extract links from 1 page of results\n",
    "def extractLinks(soup):\n",
    "    links = []\n",
    "    searchLinks = soup.find_all('div', class_ = 'yuRUbf')\n",
    "    for h in searchLinks:\n",
    "        link = h.a.get('href')\n",
    "        links.append(link)\n",
    "    return links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5736a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTitles(soup):\n",
    "    titles = [] \n",
    "    searchTitles = soup.find_all('h3', class_='LC20lb MBeuO DKV0Md')\n",
    "    for h in searchTitles:\n",
    "        titles.append(h.text)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69b85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to extract texts from 1 page of results\n",
    "def extractTexts(soup):\n",
    "    texts = []\n",
    "    searchText = soup.find_all('div', class_='VwiC3b yXK7lf MUxGbd yDYNvb lyLwlc lEBKkf')\n",
    "    for h in searchText:\n",
    "        fullText = h.text\n",
    "        try:\n",
    "            splitText = fullText.split('â€” ', 1)\n",
    "            text = splitText[1]\n",
    "            texts.append(text);\n",
    "        except:\n",
    "            texts.append(fullText)\n",
    "    return texts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152b8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateLiklihoods(links, keywords):\n",
    "    #keywords = ['personalized', 'personalization', 'machine-learning', 'AI', 'Artificial Intelligence', '24/7', 'democratizing']\n",
    "    liklihoods = []\n",
    "    detectedWordsAll = []\n",
    "    for url in links: \n",
    "        detectedWords = []\n",
    "        output = ''\n",
    "        count = 0\n",
    "        liklihood = 0\n",
    "\n",
    "        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        html_page = res.content\n",
    "        soup = bs(html_page, 'html.parser')\n",
    "        text = soup.find_all(text=True)\n",
    "\n",
    "        #ignore = ['[document]', 'a', 'article', 'label','div','script', 'style', 'title', 'img', 'svg', 'ul', 'g', 'footer', 'button', 'clippath', 'nav']\n",
    "        ignore1 = ['[document]', 'label','div','script', 'style', 'img', 'svg', 'ul', 'g', 'footer', 'button', 'clippath', 'nav']\n",
    "       \n",
    "        for t in text:\n",
    "            if t.parent.name not in ignore1:\n",
    "                output += '{} '.format(t)\n",
    "\n",
    "        # analyze the webpage to detect keywords \n",
    "        outputSub = output.split(' ')\n",
    "        #print(outputSub)\n",
    "        for word in keywords: \n",
    "            if (word in outputSub or word.capitalize() in outputSub):\n",
    "                #print('\"' + word +'\"' + ' detected')\n",
    "                detectedWords.append(word)\n",
    "                count +=1 \n",
    "            #else:\n",
    "                #print(word + ' is not there')\n",
    "        liklihood = round((count / liklihoodDenom), 2)\n",
    "        liklihoods.append(liklihood)\n",
    "        detectedWordsAll.append(detectedWords)\n",
    "        #detectedWordsAll = ', '.join(str(keyword) for keyword in detectedWordsAll)\n",
    "\n",
    "    return liklihoods, detectedWordsAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d48846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to add 1 page of results to pageInfo list\n",
    "def addToPageInfo(titles, links, texts, liklihoods, detectedWordsAll):\n",
    "    index = 0;\n",
    "    while (index < maxResult):\n",
    "        # create new dictionary of each search results' title, link, and text\n",
    "        if (liklihoods == None):\n",
    "            pageInfo.append({\"Title\": titles[index], \"Link\": links[index], \"About\": texts[index]})\n",
    "        else:\n",
    "            \n",
    "            detectedWords = str(detectedWordsAll[index])\n",
    "            print(\"detectedWordsString: \" + detectedWords)\n",
    "            pageInfo.append({\"Liklihood\": liklihoods[index], \"Title\": titles[index], \"Link\": links[index], \"About\": texts[index],  \"DetectedWords\": detectedWords})\n",
    "        index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f162f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportCSV(df):\n",
    "    # convert pageInfo to pandas dataframe and export as csv \n",
    "    df.to_csv(pathToFile + 'WebScrapeDeliverable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da76921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: reads in values from DigitalHealthWebscrape google sheet\n",
    "# Need spreadsheet ID and the cell address where the value should be read in \n",
    "# returns the value \n",
    "\n",
    "def readinValue(cellLocation):\n",
    "    sh = gc.open_by_key(spreadsheet_id)\n",
    "    worksheet = sh.get_worksheet(0)\n",
    "    value = worksheet.acell(cellLocation).value\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e61599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readWebsites():\n",
    "    links = []\n",
    "    names = []\n",
    "    row = 1;\n",
    "    isDone = False\n",
    "    while (isDone == False):\n",
    "        cellLocationURL = 'B' + str(row)\n",
    "        cellLocationName = 'D' + str(row)\n",
    "        print(\"cellLocationURL: \" + cellLocationURL, \"cellLocationName: \" + cellLocationName)\n",
    "        siteURL = readinValue(cellLocationURL)\n",
    "        links.append(siteURL)\n",
    "        websiteName = readinValue(cellLocationName)\n",
    "        names.append(websiteName)\n",
    "        #print(siteURL, websiteName)\n",
    "        if(siteURL == None and websiteName == None):\n",
    "            isDone = True\n",
    "            break\n",
    "        else:\n",
    "            row += 1 \n",
    "    return links, names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd863ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCalculateLiklihood():\n",
    "    #print(links, names)\n",
    "    cellLocationLiklihood = 'F1'\n",
    "    calculateLiklihoodInput = readinValue(cellLocationLiklihood)\n",
    "    if (calculateLiklihoodInput == 'Yes'):\n",
    "        calculateLiklihood = True\n",
    "    else:\n",
    "        calculateLiklihood = False\n",
    "    return calculateLiklihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29793eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readKeyWords():\n",
    "    keywords = []\n",
    "    row = 1;\n",
    "    isDone = False\n",
    "    while (isDone == False):\n",
    "        cellLocationKeyWords = 'H' + str(row)\n",
    "        keyword = readinValue(cellLocationKeyWords)\n",
    "        keywords.append(keyword)\n",
    "        if(keyword == None):\n",
    "            isDone = True\n",
    "            break\n",
    "        else:\n",
    "            row += 1 \n",
    "    keywords = keywords[0:len(keywords)-1]\n",
    "    global liklihoodDenom \n",
    "    liklihoodDenom= len(keywords) - 2\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af8c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInput():\n",
    "    links, names = readWebsites()\n",
    "    calculateLiklihood = readCalculateLiklihood()\n",
    "    keywords = readKeyWords()\n",
    "    return links, names, calculateLiklihood, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcc16211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSpreadSheet(df):\n",
    "    sh = gc.open_by_key(spreadsheet_id)\n",
    "    worksheet = sh.get_worksheet(1)\n",
    "    worksheet.update([df.columns.values.tolist()] + df.values.tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c073c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cellLocationURL: B1 cellLocationName: D1\n",
      "cellLocationURL: B2 cellLocationName: D2\n",
      "cellLocationURL: B3 cellLocationName: D3\n",
      "cellLocationURL: B4 cellLocationName: D4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links: ['https://www.babylonhealth.com/en-us', 'https://www.concertai.com/predictivepatient/', 'https://kaiahealth.com/', None]\n",
      "names: ['Babylon Health', 'Predictive Patient', 'Kaia Health', None]\n",
      "calculateLiklihood: True\n",
      "keywords: ['AI', 'personalized', 'artificial intelligence', 'democratize', '24/7', 'personalization', 'machine-learning']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Driver [/Users/zacharywong/.wdm/drivers/chromedriver/mac64/98.0.4758.102/chromedriver] found in cache\n",
      "/var/folders/gl/jgk7f2w528qgjggck6glqp200000gn/T/ipykernel_81259/2680139020.py:17: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "length of links list: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gl/jgk7f2w528qgjggck6glqp200000gn/T/ipykernel_81259/2680139020.py:35: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  searchBar = driver.find_element_by_name('q')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detectedWordsString: ['AI', 'personalized', '24/7']\n",
      "index: 1\n",
      "length of links list: 3\n"
     ]
    }
   ],
   "source": [
    "linksInput, namesInput, calculateLiklihood, keywords = readInput()\n",
    "print(\"links: \" + str(linksInput))\n",
    "print(\"names: \" + str(namesInput))\n",
    "print(\"calculateLiklihood: \" + str(calculateLiklihood))\n",
    "print(\"keywords: \" + str(keywords))\n",
    "# input for keyword, number of pages to scrape, and website URL \n",
    "# only want top result so only search for 1 result \n",
    "#nPages = 1\n",
    "maxResult = 1\n",
    "googleurl = 'https://www.google.com/'\n",
    "useURL = False\n",
    "useName = False\n",
    "index = 0\n",
    "waitTime = 5\n",
    "\n",
    "# Access chromedriver and determine path \n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "pathToFile = '/Users/zacharywong/github/zacharywong2023/DigitalHealthWebscrape/Deliverables/'\n",
    "\n",
    "# list of dictionaries with key/value pairs: title, link, text\n",
    "# Contains all information for all search results \n",
    "pageInfo = []\n",
    "\n",
    "\n",
    "# set the keyword you want to search for depending on whether link or name is given\n",
    "# we find the search bar using it's name attribute value\n",
    "\n",
    "\n",
    "while (index < len(linksInput)-1):\n",
    "    print(\"index: \" + str(index))\n",
    "    print(\"length of links list: \" + str((len(linksInput)-1)))\n",
    "    siteURL = linksInput[index]\n",
    "    websiteName = namesInput[index]\n",
    "    driver.get(googleurl)\n",
    "    searchBar = driver.find_element_by_name('q')\n",
    "    \n",
    "    # Booleans\n",
    "    if (siteURL != None):\n",
    "        useURL = True; \n",
    "        useName = False; \n",
    "    else:\n",
    "        useURL = False; \n",
    "        useName = True; \n",
    "\n",
    "    # first we send our keyword to the search bar followed by the enter # key depending on using URL or website name \n",
    "\n",
    "    if (useURL):\n",
    "        query = \"site: \" + siteURL\n",
    "        try:\n",
    "            searchBar.send_keys(query)\n",
    "            searchBar.send_keys('\\n')\n",
    "        except Exception as e : \n",
    "            WebDriverWait(driver, waitTime).until(EC.presence_of_element_located((By.NAME, 'q')))\n",
    "            searchBar.send_keys(query)\n",
    "            searchBar.send_keys('\\n')\n",
    "    else:\n",
    "        query = websiteName\n",
    "        searchBar.send_keys(query)\n",
    "        searchBar.send_keys('\\n')\n",
    "\n",
    "  \n",
    "\n",
    "    #capture links, header, and text\n",
    "    #pageInfo is a list of dictionaries for each page with keys/value pairs: header, link, text\n",
    "    # extract and load each page of results to pageInfo \n",
    "\n",
    "    for page in range(0, 1):\n",
    "        soup = bs(driver.page_source, 'html.parser')\n",
    "        links = extractLinks(soup);\n",
    "        texts = extractTexts(soup);\n",
    "        titles = extractTitles(soup);\n",
    "        if (calculateLiklihood):\n",
    "            liklihoods, detectedWordsAll = calculateLiklihoods(links, keywords)\n",
    "            #print('detectedWordsAllStringForm: ' + str(detectedWordsAll))\n",
    "            #detectedWordsAll = ', '.join([str(keyword) for keyword in detectedWordsAll])\n",
    "            #print(\"detectedWordsAll String Form: \" + detectedWordsAll)\n",
    "            addToPageInfo(titles, links, texts, liklihoods, detectedWordsAll)\n",
    "        else:\n",
    "            addToPageInfo(titles, links, texts, liklihoods = None, detectedWordsAll = None)\n",
    "    #print(pageInfo)\n",
    "    index += 1\n",
    "\n",
    "df = pd.DataFrame(pageInfo)\n",
    "#print(df.columns.values.tolist())\n",
    "#print(df.values.tolist())\n",
    "print(pageInfo)\n",
    "exportCSV(df) \n",
    "updateSpreadSheet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f5bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4553b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65104f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
